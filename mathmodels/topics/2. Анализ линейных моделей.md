$\newcommand{\mA}{\mathbf{A}}$
$$\newcommand{\mB}{\mathbf{b}}
\newcommand{\mE}{\mathbf{E}}$$
Анализируем линейные динамические модели с постоянной матрицей
$$\frac{d\mathbf{x}}{dt}=\mathbf{Ax}+\mathbf{b}$$
## Основные этапы
1. Получение решения (динамического) + получение стационарного решения и анализ его устойчивости
2. Оценка наблюдаемости и роли отдельных составляющих решения
3. Оценка чувствительности к параметрам
4. Решение задачи параметрической идентификации
5. Решение задач управления (выбора оптимальных значений параметров)
## 1а. Получение стационарного решения
Решаем $\mathbf{Ax}+\mathbf{b}=0$; $\mathbf{x}^*=-\mathbf{A}^{-1}\mathbf{b}$. Например, с помощью ***DECOMP*** и ***SOLVE***.
Анализ устойчивости выполняется на основе ***QR***-алгоритма ($Re\lambda_k < 0$ для асимптотической устойчивости).
## 1. Получение динамического решения
Можно воспользоваться ***RKF45***, но эффективнее учитывать линейные свойства. Точное решение:
$$\mathbf{x}(t)=e^{\mathbf{A}t}\mathbf{x}_0+\int_{0}^{t}e^{\mathbf{A}\tau}d\tau \times \mathbf{b}$$
Запишем его в точке $t_{n+1}=t_n+H$ и вычтем $e^{\mathbf{A}H}\mathbf{x}(t_n)$, $H$ — шаг наблюдения решения. Первое слагаемое сокращается.
$$\mathbf{x}(t_n+H)-e^{\mathbf{A}H}\mathbf{x}(t_n)=\left(\int_0^{t_n+H}e^{\mathbf{A}\tau}d\tau-e^{\mathbf{A}H}\int_0^{t_n}e^{\mathbf{A}\tau}d\tau\right)\times\mathbf{b}$$
Переносим слагаемое в правую часть и вносим $e^{\mathbf{A}H}$ под интеграл.
$$\mathbf{x}(t_n+H)=e^{\mathbf{A}H}\mathbf{x}(t_n)+\left(\int_0^{t_n+H}e^{\mathbf{A}\tau}d\tau-\int_H^{t_n+H}e^{\mathbf{A}\tau}d\tau\right)\times\mathbf{b}$$
Вычитаем интегралы.
$$\mathbf{x}(t_n+H)=e^{\mathbf{A}H}\mathbf{x}(t_n)+\int_0^{H}e^{\mathbf{A}\tau}d\tau\times\mathbf{b}$$
Эта формула позволяет *однократно* посчитать матричную экспоненту и интеграл, так как они не зависят от $t$, а затем использовать пошаговый метод.
##### Вычисление матричной экспоненты
Для вычисления матричной экспоненты и интеграла воспользуемся матричным степенным разложением.
$$e^{\mA H}\approx
\mathbf{E}+H\mA+\frac{H^2\mA^2}{2}+\dots \quad
\int_0^He^{\mA\tau}d\tau\approx 
H\left(\mathbf{E}+\frac{H\mA}{2}+\frac{H^2\mA^2}{6}+\dots\right)$$
Для удовлетворительной скорости сходимости необходимо обеспечить выполнение $\left|\lambda_k\right|_{max}H<1$ или $||\mA||H<1$.

Если невозможно обеспечить выполнение этого условия (система жёсткая), то по желаемому значению $H$ выбирается такое целое $N$, что для $h=\frac{H}{2^N}$ это условие выполняется. Далее строится $e^{\mathbf{A}h}$ и $N$ раз возводится в квадрат, достигая матрицы $e^{\mathbf{A}H}$.

Аналогичная формула для вектора (второго слагаемого)
$$\mathbf{g}(h)=
\int_0^he^{\mA\tau}d\tau\times\mB \quad 
\mathbf{g}(2h)=
\left(\mathbf{E}+e^{\mA h}\right)\mathbf{g}(h)$$
Для доказательства делим интеграл на две части и делаем замену переменной во втором.
$$\begin{align}
\mathbf{g}(2h)=&
\int_0^{2h}e^{\mA\tau}d\tau\times\mB=
\int_0^he^{\mA\tau}d\tau\times\mB+
\int_h^{2h}e^{\mA\tau}d\tau\times\mB=\\
=&\int_0^he^{\mA\tau}d\tau\times\mB+
e^{\mA h}\int_0^he^{\mA\tau}d\tau\times\mB=
(\mathbf{E}+e^{\mA h})\mathbf{g}(h)
\end{align}$$
##### Алгоритм
1. Задаём желаемое $H$. Выбираем $N$ так, что
$$h=\frac{H}{2^N}<\frac1{||\mA||}$$
2. Для $h$ строим матричную экспоненту $e^{\mA h}$ и вектор $\mathbf{g}(h)$
3. Используем формулы удвоения шага и получаем $e^{\mA H}$ и $\mathbf{g}(H)$
4. Решаем уравнение пошаговым методом
##### Программа LSODE
Этот алгоритм реализован в программе 
***LSODE(N, H, CH, A, B, X, EAH, SL, INDEX)***, где
***N*** — размерность системы,
***H*** — желаемый шаг,
***CH*** — для оценки шага $h$ (0.1..5.0),
***A, B*** — матрица и вектор системы,
***X*** — вектор решения,
***EAH*** — $e^{\mA H}$,
***SL*** — рабочий массив,
***INDEX*** — управляющий параметр.

Если игнорировать ошибки округления, то все вычисления, кроме разложения, точные. Разложить можно до ошибки округления.
## 2. Определение наблюдаемости отдельных составляющих решения
##### Формула Лагранжа-Сильвестра
Пусть
$$\begin{align}
\mA \mathbf{u}_k&=\lambda_k\mathbf{u}_k \\
\mA^{-1}\mathbf{v}_i&=\lambda_i\mathbf{v}_i
\end{align}
$$
Транспонируем последнюю формулу.
$$\mathbf{v}_i^T\mA=\lambda_i\mathbf{v}_i^T$$
Первую формулу умножаем слева на $\mathbf{v}_i^T$, а третью справа на $\mathbf{u}_k$, вычитаем.
$$0=(\lambda_k-\lambda_i)\mathbf{v}_i^T\mathbf{u}_k$$
Если $k\neq i$, то
$$\mathbf{v}_i^T\mathbf{u}_k=0$$
Дополнительно нормируем эти векторы так, что
$$\mathbf{v}_k^T\mathbf{u}_k=1$$
Формула Лагранжа-Сильвестра
$$\begin{align}
\mathbf{f}(\mathbf{A})&=\sum_{k=1}^{m}\mathbf{T}_kf(\lambda_k)\\
\mathbf{T}_k&=\frac
{
(\mA-\lambda_1\mE)\dots(\mA-\lambda_{k-1}\mE)
(\mA-\lambda_{k+1}\mE)\dots(\mA-\lambda_{m}\mE)
}
{
(\lambda_k-\lambda_1)\dots(\lambda_k-\lambda_{k-1})
(\lambda_k-\lambda_{k+1})\dots(\lambda_k-\lambda_{m})
}
\end{align}
$$
Для чужих собственных векторов $i\neq k$ (так как $(\mA-\lambda_i\mE)\times \mathbf{u}_i=\mathbf{0}$)
$$\mathbf{T}_k\mathbf{u}_i=0$$
Для своих собственных векторов (умножить все множители на $\mathbf{u}_k$)
$$\mathbf{T}_k\mathbf{u}_k=\mathbf{u}_k$$
Разложим каждую строку
$$\mathbf{T}_k=\sum_{j=1}^m\mathbf{c}_j\mathbf{v}_j^T$$
Если $k\neq i$ 
$$\mathbf{T}_k\mathbf{u}_i=
\sum_{j=1}^m(\mathbf{c}_j\mathbf{v}_j^T) \mathbf{u}_i=
\sum_{j=1}^m\mathbf{c}_j(\mathbf{v}_j^T \mathbf{u}_i)=
\mathbf{c}_i(\mathbf{v}_i^T \mathbf{u}_i)=\mathbf{c}_i=\mathbf{0}$$
То есть
$$\mathbf{T}_k=\mathbf{c}_k\mathbf{v}_k^T$$
Подставляем это в выражение со своими собственными векторами
$$(\mathbf{c}_k\mathbf{v}_k^T)\mathbf{u}_k=
\mathbf{c}_k(\mathbf{v}_k^T\mathbf{u}_k)=
\mathbf{c}_k=\mathbf{u}_k$$
Теперь формулу Лагранжа-Сильвестра можно записать так
$$\mathbf{f}(\mA)=\sum_{k=1}^m \mathbf{u}_k\mathbf{v}_k^Tf(\lambda_k)$$
